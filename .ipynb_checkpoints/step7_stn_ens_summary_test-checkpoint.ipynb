{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save statistics\n",
      "Done\n",
      "0:00:03.067909\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "# This script is used to calculate some summary statistics of yearly ensemble.\n",
    "\n",
    "import numpy as np\n",
    "import argparse, os\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import netCDF4 as nc\n",
    "\n",
    "startTime = datetime.datetime.now()\n",
    "\n",
    "def process_command_line():\n",
    "    '''Parse the commandline'''\n",
    "    parser = argparse.ArgumentParser(description='Script to subset a netcdf file based on a list of IDs.')\n",
    "    parser.add_argument('EnsDirBase', help='parent path of ens data.')\n",
    "    parser.add_argument('EnsFolder',help='ens folder name.')\n",
    "    parser.add_argument('yr',help='year.')\n",
    "    parser.add_argument('startEns',help='start ensemble member id.')\n",
    "    parser.add_argument('stopEns',help='end ensemble member id.')\n",
    "    parser.add_argument('lb_perct',help='percentile value for lower bound.')\n",
    "    parser.add_argument('ub_perct',help='percentile value for upper bound.')\n",
    "    args = parser.parse_args()\n",
    "    return(args)\n",
    "\n",
    "#======================================================================================================\n",
    "# main script\n",
    "# # process command line\n",
    "# args = process_command_line()\n",
    "# EnsDirBase = args.EnsDirBase\n",
    "# EnsFolder = args.EnsFolder\n",
    "\n",
    "# yr=int(args.yr)\n",
    "# startEns=int(args.startEns)   \n",
    "# stopEns=int(args.stopEns)  \n",
    "# ens_num = (stopEns-startEns+1)\n",
    "\n",
    "# lb_perct=int(args.lb_perct)\n",
    "# ub_perct=int(args.ub_perct)\n",
    "\n",
    "EnsDirBase='/glade/u/home/hongli/scratch/2020_04_21nldas_gmet/data'\n",
    "EnsFolder='stn_ens' \n",
    "\n",
    "yr=2015 \n",
    "startEns=1 \n",
    "stopEns=3 \n",
    "ens_num = (stopEns-startEns+1)\n",
    "\n",
    "lb_perct=5 \n",
    "ub_perct=95\n",
    "\n",
    "lb_perct_str = str(lb_perct)\n",
    "ub_perct_str = str(ub_perct)\n",
    "\n",
    "EnsDir = os.path.join(EnsDirBase, EnsFolder)\n",
    "outdir = os.path.join(EnsDirBase, EnsFolder+'_summary')\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "#=================================================================================\n",
    "# read ensemble data\n",
    "print('read ensemble data')\n",
    "for i in range(ens_num):\n",
    "    NUM = startEns+i\n",
    "    ens_file = os.path.join(EnsDir, 'conus_daily_eighth_'+ str(yr) + '0101_' + str(yr) + '1231_'+ str('%03d' % (NUM)) +'.nc4')\n",
    "\n",
    "    f=xr.open_dataset(ens_file)\n",
    "    time = f['time'][:]\n",
    "    pcp = f.variables['pcp'][:]\n",
    "    tmean = f.variables['t_mean'][:]\n",
    "    trange = f.variables['t_range'][:]\n",
    "\n",
    "    if NUM == startEns: # create ens array for one member\n",
    "        (nt,ny,nx) = np.shape(pcp)\n",
    "        pcp_ens = np.zeros((nt,ny,nx,ens_num))\n",
    "        tmean_ens = np.zeros((nt,ny,nx,ens_num))\n",
    "        trange_ens = np.zeros((nt,ny,nx,ens_num))\n",
    "\n",
    "    pcp_ens[:,:,:,i] = pcp\n",
    "    tmean_ens[:,:,:,i] = tmean\n",
    "    trange_ens[:,:,:,i] = trange\n",
    "\n",
    "#=================================================================================\n",
    "# calculate ensemble statistics. (time,y,x)\n",
    "print('calculate ensemble statistics')\n",
    "print('pcp')\n",
    "pcp_ens_mean = np.nanmean(pcp_ens, axis = 3)\n",
    "pcp_ens_median = np.nanmedian(pcp_ens, axis = 3)\n",
    "pcp_ens_std = np.std(pcp_ens, axis = 3)\n",
    "pcp_ens_lb = np.percentile(pcp_ens, lb_perct, axis = 3)\n",
    "pcp_ens_ub = np.percentile(pcp_ens, ub_perct, axis = 3)\n",
    "del pcp_ens\n",
    "\n",
    "print('tmean')\n",
    "tmean_ens_mean = np.nanmean(tmean_ens, axis = 3)\n",
    "tmean_ens_median = np.nanmedian(tmean_ens, axis = 3)\n",
    "tmean_ens_std = np.std(tmean_ens, axis = 3)\n",
    "tmean_ens_lb = np.percentile(tmean_ens, lb_perct, axis = 3)\n",
    "tmean_ens_ub = np.percentile(tmean_ens, ub_perct, axis = 3)\n",
    "del tmean_ens\n",
    "\n",
    "print('trange')\n",
    "trange_ens_mean = np.nanmean(trange_ens, axis = 3)\n",
    "trange_ens_median = np.nanmedian(trange_ens, axis = 3)\n",
    "trange_ens_std = np.std(trange_ens, axis = 3)\n",
    "trange_ens_lb = np.percentile(trange_ens, lb_perct, axis = 3)\n",
    "trange_ens_ub = np.percentile(trange_ens, ub_perct, axis = 3)\n",
    "del trange_ens\n",
    "\n",
    "#=================================================================================\n",
    "#save statistics summary\n",
    "print('save statistics')\n",
    "SrcFile=os.path.join(EnsDir, 'conus_daily_eighth_'+ str(yr) + '0101_' + str(yr) + '1231_001.nc4')\n",
    "with nc.Dataset(SrcFile) as src:   \n",
    "    DstFile = os.path.join(outdir, 'ens_forc.sumamry.'+ str(yr)+'.nc')\n",
    "    with nc.Dataset(DstFile, \"w\") as dst:\n",
    "\n",
    "        # copy dimensions\n",
    "        for name, dimension in src.dimensions.items():\n",
    "             dst.createDimension(\n",
    "                name, (len(dimension) if not dimension.isunlimited() else None))\n",
    "\n",
    "        # copy variable attributes all at once via dictionary (for the included variables)\n",
    "        include = ['latitude', 'longitude', 'time']\n",
    "        for name, variable in src.variables.items():\n",
    "            if name in include:\n",
    "                x = dst.createVariable(name, variable.datatype, variable.dimensions)               \n",
    "                dst[name].setncatts(src[name].__dict__)\n",
    "                dst[name][:]=src[name][:]                \n",
    "\n",
    "        # create summary variables \n",
    "        vars_short = ['pcp_mean','pcp_median','pcp_std','pcp_ub','pcp_lb',\n",
    "                     'tmean_mean','tmean_median','tmean_std','tmean_ub','tmean_lb',\n",
    "                     'trange_mean','trange_median','trange_std','trange_ub','trange_lb']\n",
    "        vars_long = ['Mean daily precipitation','Median daily precipitation',\n",
    "                     'Standard deviation of daily precipitation',\n",
    "                     ub_perct_str+'th percentile of daily precipitation',\n",
    "                     lb_perct_str+'th percentile of daily precipitation',\n",
    "                     'Mean daily temperature', 'Median daily temperature',\n",
    "                     'Standard deviation of daily mean temperature',\n",
    "                     ub_perct_str+'th percentile of daily mean temperature',\n",
    "                     lb_perct_str+'th percentile of daily mean temperature',\n",
    "                     'Mean daily temperature range', 'Median daily temperature range',\n",
    "                     'Standard deviation of daily temperature range',\n",
    "                     ub_perct_str+'th percentile of daily temperature range',\n",
    "                     lb_perct_str+'th percentile of daily temperature range']\n",
    "        units = ['mm/day', 'mm/day', 'mm/day', 'mm/day','mm/day',\n",
    "                 'degC', 'degC', 'degC', 'degC', 'degC',\n",
    "                 'degC', 'degC', 'degC', 'degC','degC']\n",
    "\n",
    "        for i, var in enumerate(vars_short):\n",
    "            var_i = dst.createVariable(var,np.float64,('time','lat','lon')) # note: unlimited dimension is leftmost\n",
    "            var_i.long_name = vars_long[i]\n",
    "            var_i.units = units[i] \n",
    "\n",
    "        dst.variables['pcp_mean'][:] = pcp_ens_mean\n",
    "        dst.variables['pcp_median'][:] = pcp_ens_median\n",
    "        dst.variables['pcp_std'][:] = pcp_ens_std \n",
    "        dst.variables['pcp_ub'][:] = pcp_ens_lb\n",
    "        dst.variables['pcp_lb'][:] = pcp_ens_ub \n",
    "\n",
    "        dst.variables['tmean_mean'][:] = tmean_ens_mean\n",
    "        dst.variables['tmean_median'][:] = tmean_ens_median\n",
    "        dst.variables['tmean_std'][:] = tmean_ens_std \n",
    "        dst.variables['tmean_ub'][:] = tmean_ens_lb\n",
    "        dst.variables['tmean_lb'][:] = tmean_ens_ub \n",
    "\n",
    "        dst.variables['trange_mean'][:] = trange_ens_mean\n",
    "        dst.variables['trange_median'][:] = trange_ens_median\n",
    "        dst.variables['trange_std'][:] = trange_ens_std \n",
    "        dst.variables['trange_ub'][:] = trange_ens_lb\n",
    "        dst.variables['trange_lb'][:] = trange_ens_ub \n",
    "            \n",
    "print('Done')\n",
    "print(datetime.datetime.now()-startTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((365, 224, 464, 3), (365, 224, 464))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(pcp_ens),np.shape(pcp_ens_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                nan],\n",
       "        [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                nan],\n",
       "        [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                nan],\n",
       "        ...,\n",
       "        [0.1       , 0.1       , 0.17772385, ..., 1.78561664, 0.40108433,\n",
       "         0.64557612],\n",
       "        [0.34056649, 0.1       , 0.1555008 , ..., 0.10929642, 0.12778546,\n",
       "         0.1       ],\n",
       "        [0.36982411, 1.25603187, 0.37433627, ..., 1.0359112 , 0.40113106,\n",
       "         0.1       ]]), 3.333457072575887)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcp_ens[0,:,:,0],pcp_ens_mean[0,200,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3334570733333333"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2.5307498+ 2.63094163+ 4.83867979)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[2.5307498 , 2.63094163, 4.83867979],\n",
       "         [1.64616776, 1.04680431, 4.50561953],\n",
       "         [0.96762699, 0.96605194, 2.53194118]],\n",
       "\n",
       "        [[1.69602013, 1.37978685, 6.34213686],\n",
       "         [1.21125102, 1.2239244 , 5.67758608],\n",
       "         [0.99794692, 0.84607494, 3.14887786]],\n",
       "\n",
       "        [[1.1934967 , 2.14177108, 5.92372131],\n",
       "         [1.12614596, 1.77862322, 3.73518395],\n",
       "         [0.65913612, 2.38473797, 4.53950787]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcp_ens[0:1,200:203,200:203,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ENTER]",
   "language": "python",
   "name": "conda-env-ENTER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
